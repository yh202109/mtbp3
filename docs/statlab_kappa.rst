#############
StatLab/Cohen's Kappa for inter-rater reliability
#############

*************
Background
*************

Cohen's kappa is a statistic used for describing, summarizing, estimating and testing inter-ratter consistency. 


*************
Notation
*************

For two categories rating, assume :math:`Y_{r,i} \in \{A,B\}` for rater :math:`r=1,2` and sample index :math:`i = 1, \ldots, n`.

.. csv-table:: counts
   :header: " ", "R2:A", "R2:B", "Total"
   :widths: 10 10 10 10

   "R1: A", :math:`N_{11}`, :math:`N_{12}`, :math:`N_{1\bullet}`
   "R1: B", :math:`N_{21}`, :math:`N_{22}`, :math:`N_{2\bullet}`
   "Total", :math:`N_{\bullet 1}`, :math:`N_{\bullet 2}`, :math:`N_{\bullet \bullet}` 

test1

.. list-table:: counts2
   :widths: 10 10 10 10
   :header-rows: 1

   * - R1: A 
     - :math:`N_{11}`
     - :math:`N_{12}` 
     - :math:`N_{1\bullet}` 
   * - R1: A 
     - :math:`N_{11}`
     - :math:`N_{12}` 
     - :math:`N_{1\bullet}` 

test2